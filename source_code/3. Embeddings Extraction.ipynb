{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x1bae83cf230>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import v2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from functions import LoadEncoderDecoder, progress\n",
    "from create_pytorch_dataset import CustomImageDataset\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## VARIABLES ##########\n",
    "encoder_dictionary_filename = 'image_decoder.pkl'   # File to save the encoder and decoder\n",
    "datasource_filepath = 'cleaned_data/training_data.csv'    # File to save the training data\n",
    "cleaned_img_dir = 'cleaned_data/images/'  # Folder containing the cleaned images\n",
    "final_size = 224    # Final size of the images\n",
    "batch_size = 64    # Batch size for the DataLoader, \n",
    "\n",
    "# Paths to save the training, validation and test datasets\n",
    "train_indices_file = 'datasets/train_indices.pt'\n",
    "val_indices_file = 'datasets/val_indices.pt'\n",
    "test_indices_file = 'datasets/test_indices.pt'\n",
    "\n",
    "# Final model\n",
    "best_model_weights_path = 'best_model_params.pt'    # Path to the best model weights\n",
    "final_model_dir = 'final_model' # Directory to save the final model\n",
    "final_model_weights = 'image_model.pt' # Name of the final model weights file\n",
    "\n",
    "# Embedings\n",
    "model_weights_path = f'{final_model_dir}/{final_model_weights}'  # Path to the model weights                        \n",
    "embeddings_output_path = 'image_embeddings.json'    # Path to save the image embeddings\n",
    "\n",
    "# Data transformations\n",
    "data_transforms = {\n",
    "    'train': v2.Compose([\n",
    "        v2.ToImage(), # Convert to tensor, only needed for PIL images\n",
    "        v2.RandomResizedCrop(size=(final_size, final_size), antialias=True),\n",
    "        v2.RandomHorizontalFlip(p=0.5),\n",
    "        # v2.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "        # v2.RandomRotation(20),\n",
    "        v2.ToDtype(torch.float32, scale=True), # this has replaced ToTensor()\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.255]),\n",
    "    ]),\n",
    "    'val': v2.Compose([\n",
    "        v2.ToImage(), # Convert to tensor, only needed for PIL images\n",
    "        v2.Resize(256),\n",
    "        v2.CenterCrop(final_size),\n",
    "        v2.ToDtype(torch.float32, scale=True), # this has replaced ToTensor()\n",
    "        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': v2.Compose([\n",
    "        v2.ToImage(),  # Convert to tensor, only needed for PIL images\n",
    "        v2.Resize(256),  # Resize to a standard size\n",
    "        v2.CenterCrop(final_size),  # Center crop to the final input size\n",
    "        v2.ToDtype(torch.float32, scale=True),  # Convert to tensor with dtype\n",
    "        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with the same mean and std as training\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## Loading encoder and decoder ##########\n",
      "\n",
      "\n",
      "########## Load Decoder Dictionary ##########\n",
      "----> Decoder dictionary loaded successfully\n",
      "----> Encoder and decoder extracted from the dictionary\n",
      "----> Encoder:\n",
      " {'Home & Garden': 0, 'Baby & Kids Stuff': 1, 'DIY Tools & Materials': 2, 'Music, Films, Books & Games': 3, 'Phones, Mobile Phones & Telecoms': 4, 'Clothes, Footwear & Accessories': 5, 'Other Goods': 6, 'Health & Beauty': 7, 'Sports, Leisure & Travel': 8, 'Appliances': 9, 'Computers & Software': 10, 'Office Furniture & Equipment': 11, 'Video Games & Consoles': 12}\n",
      "----> Decoder:\n",
      " {0: 'Home & Garden', 1: 'Baby & Kids Stuff', 2: 'DIY Tools & Materials', 3: 'Music, Films, Books & Games', 4: 'Phones, Mobile Phones & Telecoms', 5: 'Clothes, Footwear & Accessories', 6: 'Other Goods', 7: 'Health & Beauty', 8: 'Sports, Leisure & Travel', 9: 'Appliances', 10: 'Computers & Software', 11: 'Office Furniture & Equipment', 12: 'Video Games & Consoles'}\n"
     ]
    }
   ],
   "source": [
    "# Load the encoder and decoder\n",
    "print('\\n########## Loading encoder and decoder ##########')\n",
    "encoder, decoder = LoadEncoderDecoder(encoder_dictionary_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Initialising the model ##########\n",
      "----> Model loaded successfully\n",
      "----> Default number of features in the model: 2048\n",
      "----> Required number of classes in the model: 13\n",
      "----> Final linear layer replaced with required number of classess\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model architecture\n",
    "print('########## Initialising the model ##########')\n",
    "# Load the pre-trained ResNet-50 model with the 'weights' parameter\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT) # Best available weights (currently alias for IMAGENET1K_V2)\n",
    "print('----> Model loaded successfully')\n",
    "\n",
    "# Disable gradients on all model parameters to freeze the weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get the number of input features for the final linear layer\n",
    "num_features = model.fc.in_features\n",
    "print(f'----> Default number of features in the model: {num_features}')\n",
    "\n",
    "# Replace the final linear layer with a new one (`num_classes` is the number of categories)\n",
    "num_classes = len(encoder)  # Get the number of categories\n",
    "print(f'----> Required number of classes in the model: {num_classes}')\n",
    "\n",
    "# Replace the final linear layer\n",
    "\n",
    "# Adding dropout in a fully connected layer\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 1024),  # First linear layer (reduce dimensions)\n",
    "    nn.ReLU(),                     # ReLU activation\n",
    "    nn.Dropout(p=0.5),             # Dropout layer to prevent overfitting\n",
    "    nn.Linear(1024, num_classes)    # Final linear layer to match the number of classes\n",
    ")\n",
    "  \n",
    "print(f'----> Final linear layer replaced with required number of classess')\n",
    "\n",
    "\n",
    "# Load the best model parameters\n",
    "if torch.cuda.is_available():\n",
    "    model.load_state_dict(torch.load(best_model_weights_path, weights_only=True))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(best_model_weights_path, weights_only=True, map_location=torch.device('cpu')))\n",
    "\n",
    "# Replace the final linear layer with a new one (1000 neurons for feature extraction)\n",
    "model.fc = torch.nn.Linear(num_features, 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Model moved to cuda\n",
      "  ----> Current device number is: 0\n",
      "  ----> GPU name is: NVIDIA GeForce RTX 3060 Ti\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # Use the GPU if available\n",
    "model = model.to(device)    # Move the model to the device\n",
    "print(f\"----> Model moved to {device}\")   # Print the device\n",
    "if torch.cuda.is_available():\n",
    "    devNumber = torch.cuda.current_device() # Get the current device number\n",
    "    print(f\"  ----> Current device number is: {devNumber}\") # Print the current device number\n",
    "    devName = torch.cuda.get_device_name(devNumber) # Get the device name\n",
    "    print(f\"  ----> GPU name is: {devName}\")    # Print the device name\n",
    "    \n",
    "print('-' * 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the final model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Final model weights saved to final_model\\image_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the final model weights\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "final_model_weights_path = os.path.join(final_model_dir, final_model_weights)\n",
    "torch.save(model.state_dict(), final_model_weights_path)\n",
    "print(f\"----> Final model weights saved to {final_model_weights_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Custom Image Dataset ##########\n",
      "----> Custom Image Dataset created\n",
      "----> Number of samples in the dataset: 12604\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "print('########## Custom Image Dataset ##########')\n",
    "dataset = CustomImageDataset(datasource_file=datasource_filepath, img_dir=cleaned_img_dir, transform=data_transforms['val'])\n",
    "print('----> Custom Image Dataset created')\n",
    "print(f\"----> Number of samples in the dataset: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "########## Creating DataLoader ##########\n",
      "----> DataLoader created successfully\n",
      "----> Number of batches in the DataLoader: 197\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n########## Creating DataLoader ##########')\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "print('----> DataLoader created successfully')\n",
    "print(f\"----> Number of batches in the DataLoader: {len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Total of 12604 image embeddings saved to image_embeddings.json%  [12604 / 12604]\n"
     ]
    }
   ],
   "source": [
    "# Extract embeddings\n",
    "image_embeddings = {}\n",
    "\n",
    "current_image = 0   # Counter to keep track of the number of images processed\n",
    "no_of_images = len(dataset)\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader:\n",
    "        \n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        \n",
    "        \n",
    "        for idx, output in enumerate(outputs):\n",
    "            try:\n",
    "                image_id = dataset.img_labels.iloc[current_image, 1] # Use current_image as the index\n",
    "                image_embeddings[image_id] = output.tolist()  # Convert numpy array to list for JSON serialization\n",
    "                current_image += 1  # Increment the counter\n",
    "                progress(current_image, no_of_images)\n",
    "            except Exception as e:\n",
    "                print(f\"----> Image processing failed for: {dataset.img_labels.iloc[current_image, 1]}\")\n",
    "                print(f'----> Error occured when trying to filter the data: {e}')\n",
    "\n",
    "# Save embeddings dictionary as JSON\n",
    "with open(embeddings_output_path, 'w') as f:\n",
    "    json.dump(image_embeddings, f)\n",
    "\n",
    "print(f\"----> Total of {len(image_embeddings)} image embeddings saved to {embeddings_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facebook_marketplace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
